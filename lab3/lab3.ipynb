{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - load the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - take a subset of the data set (3000 for training and 1000 for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 28, 28)\n",
      "(1000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# combine the training data and testing to get the complete dataset\n",
    "x_data = np.concatenate((x_train, x_test), axis=0)\n",
    "y_data = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "test_size = 1000 / (1000 + 3000)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=test_size, random_state=0)\n",
    "\n",
    "x_train = X_train[:3000]\n",
    "y_train = Y_train[:3000]\n",
    "x_test = X_test[:1000]\n",
    "y_test = Y_test[:1000]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - perform necessary reshaping of the data for the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 784)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - initialise the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "sgd_classifier = SGDClassifier(max_iter=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - fit the model to the traing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(max_iter=250)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.fit(x_train, y_train)\n",
    "dt_classifier.fit(x_train, y_train)\n",
    "sgd_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - use the trained/fitted model to evaluate the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_knn_predict = knn_classifier.predict(x_test)\n",
    "y_dt_predict = dt_classifier.predict(x_test)\n",
    "y_sgd_predict = sgd_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - report the performance of each classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of KNN is:\n",
      "0.777\n",
      "The precision score of KNN is:\n",
      "0.7815910643829962\n",
      "The recall score of KNN is:\n",
      "0.7763271214957356\n",
      "The f1 score of KNN is:\n",
      "0.7732931062179594\n",
      "The confusion matrix of KNN:\n",
      "[[ 83   1   2   4   1   0   7   0   1   0]\n",
      " [  3 108   1   1   1   0   1   0   0   0]\n",
      " [  5   0  78   0  15   0  13   0   0   0]\n",
      " [ 12   2   1  70   5   0   3   0   1   0]\n",
      " [  0   0  17   7  61   0  16   0   0   0]\n",
      " [  0   0   0   0   0  68   0  20   0  17]\n",
      " [ 21   0  14   2   6   0  49   0   0   0]\n",
      " [  0   0   0   0   0   2   0  70   0   5]\n",
      " [  4   0   1   0   0   0   3   1  97   0]\n",
      " [  0   0   0   0   0   3   1   3   0  93]]\n"
     ]
    }
   ],
   "source": [
    "knn_accuary = metrics.accuracy_score(y_test, y_knn_predict)\n",
    "knn_precision = metrics.precision_score(y_test, y_knn_predict, average='macro')\n",
    "knn_recall = metrics.recall_score(y_test, y_knn_predict, average='macro')\n",
    "knn_f1 = metrics.f1_score(y_test, y_knn_predict, average='macro')\n",
    "knn_confusion_matrix = metrics.confusion_matrix(y_test, y_knn_predict)\n",
    "\n",
    "print('The accuracy score of KNN is:')\n",
    "print(knn_accuary)\n",
    "print('The precision score of KNN is:')\n",
    "print(knn_precision)\n",
    "print('The recall score of KNN is:')\n",
    "print(knn_recall)\n",
    "print('The f1 score of KNN is:')\n",
    "print(knn_f1)\n",
    "print('The confusion matrix of KNN:')\n",
    "print(knn_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the accuary of our KNN classifier with the scores presented in the paper\n",
    "\n",
    "Our result is worse than the accuracy in paper. When initializing the KNN classifier, we only set the parameter n_neighbors to 3, while other parameters remain the default. Therefore, when comparing our result with the results of the KNN classifiers used in the paper, we only compare our result with those generated by classifiers with default parameters other than n_neighbors (i.e. weights=uniform, p=2). \n",
    "We can find that when n_neighbors=3, the accuracy is only 0.777, when n_neighbors=5, the accuracy is 0.849, when n_neighbors=9, the accuracy is 0.847. \n",
    "\n",
    "There may be three reasons. One is that KNN is very sensitive to the selection of n_neighbors or the number of k, n_neighbors=3 is too small for this dataset. The other is that we only use part of the entire dataset, so the features may be not enough. The last one is that is that the subset we get from the entire dataset may not have exactly the same data distribution as the entire dataset used in the paper which will also affect the accuracy of the trained classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of DT is:\n",
      "0.713\n",
      "The precision score of DT is:\n",
      "0.7141606824331195\n",
      "The recall score of DT is:\n",
      "0.7093676968454571\n",
      "The f1 score of DT is:\n",
      "0.710221565751715\n",
      "The confusion matrix of DT:\n",
      "[[ 72   0   2   5   0   0  16   1   3   0]\n",
      " [  2 106   0   2   1   0   4   0   0   0]\n",
      " [  5   2  63   2  16   1  19   0   3   0]\n",
      " [ 10   3   3  66   2   0   5   1   4   0]\n",
      " [  2   0  17  14  55   0  13   0   0   0]\n",
      " [  0   2   0   0   0  82   0  15   2   4]\n",
      " [ 17   0  11   0  14   0  47   0   3   0]\n",
      " [  0   0   0   0   0  14   0  56   0   7]\n",
      " [  1   0   1   1   3   4  10   0  86   0]\n",
      " [  0   0   0   0   0   6   1  11   2  80]]\n"
     ]
    }
   ],
   "source": [
    "dt_accuary = metrics.accuracy_score(y_test, y_dt_predict)\n",
    "dt_precision = metrics.precision_score(y_test, y_dt_predict, average='macro')\n",
    "dt_recall = metrics.recall_score(y_test, y_dt_predict, average='macro')\n",
    "dt_f1 = metrics.f1_score(y_test, y_dt_predict, average='macro')\n",
    "dt_confusion_matrix = metrics.confusion_matrix(y_test, y_dt_predict)\n",
    "\n",
    "print('The accuracy score of DT is:')\n",
    "print(dt_accuary)\n",
    "print('The precision score of DT is:')\n",
    "print(dt_precision)\n",
    "print('The recall score of DT is:')\n",
    "print(dt_recall)\n",
    "print('The f1 score of DT is:')\n",
    "print(dt_f1)\n",
    "print('The confusion matrix of DT:')\n",
    "print(dt_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the accuary of our DT classifier with the scores presented in the paper\n",
    "\n",
    "Our result is worse than the accuracy in paper. When initializing the DT classifier, we keep all parameters as default values (i.e. criterion=gini, splitter=best). Therefore, when comparing our result with the results of the DT classifiers used in the paper, we only compare our result with those generated by classifiers with default parameters. We can find that the accuracy scores generated by the DT classifiers in paper are between 0.78 and 0.79.  But our accuracy is 0.713. \n",
    "\n",
    "There may be two reasons. One is that we only use part of the entire dataset, so the features may be not enough, the other is that the subset we get from the entire dataset may not have exactly the same data distribution as the entire dataset used in the paper which will also affect the accuracy of the trained classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of SGD is:\n",
      "0.769\n",
      "The precision score of SGD is:\n",
      "0.7890365416046505\n",
      "The recall score of SGD is:\n",
      "0.7666215736392156\n",
      "The f1 score of SGD is:\n",
      "0.752531024161388\n",
      "The confusion matrix of SGD:\n",
      "[[ 80   1   2  10   3   0   0   0   3   0]\n",
      " [  2 107   1   3   1   0   1   0   0   0]\n",
      " [  3   0  66   1  37   0   2   0   2   0]\n",
      " [  5   1   1  78   8   0   1   0   0   0]\n",
      " [  0   0   7   8  85   0   1   0   0   0]\n",
      " [  0   0   0   0   2  82   1   7   4   9]\n",
      " [ 16   0  13   5  41   0  15   0   2   0]\n",
      " [  0   0   0   0   0   2   0  71   1   3]\n",
      " [  2   0   0   4   3   0   0   1  96   0]\n",
      " [  0   0   0   0   0   1   0  10   0  89]]\n"
     ]
    }
   ],
   "source": [
    "sgd_accuary = metrics.accuracy_score(y_test, y_sgd_predict)\n",
    "sgd_precision = metrics.precision_score(y_test, y_sgd_predict, average='macro')\n",
    "sgd_recall = metrics.recall_score(y_test, y_sgd_predict, average='macro')\n",
    "sgd_f1 = metrics.f1_score(y_test, y_sgd_predict, average='macro')\n",
    "sgd_confusion_matrix = metrics.confusion_matrix(y_test, y_sgd_predict)\n",
    "\n",
    "print('The accuracy score of SGD is:')\n",
    "print(sgd_accuary)\n",
    "print('The precision score of SGD is:')\n",
    "print(sgd_precision)\n",
    "print('The recall score of SGD is:')\n",
    "print(sgd_recall)\n",
    "print('The f1 score of SGD is:')\n",
    "print(sgd_f1)\n",
    "print('The confusion matrix of SGD:')\n",
    "print(sgd_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the accuary of our SGD classifier with the scores presented in the paper\n",
    "\n",
    "Our result is worse than the accuracy in paper. When initializing the SGD classifier, we only set max_iter to 250 to limit the maximum number of algorithm iterations. Therefore, when comparing our result with the results of the SGD classifiers used in the paper, we only compare our result with those generated by classifiers with default parameters (loss=hinge, penalty=l2). We can find that the accuracy scores generated by the DT classifier in paper is 0.819.  But our accuracy is 0.769. \n",
    "\n",
    "\n",
    "There may be three reasons. One is that we set max_iter to 250, but the parameter used in the paper is 1000. Therefore, the number of training times of our model is much less than the model in the paper, which may lead to underfitting and lower accuracy. The other is that we only use part of the entire dataset, so the features may be not enough. The last one is that the subset we get from the entire dataset may not have exactly the same data distribution as the entire dataset used in the paper which will also affect the accuracy of the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a67019f997bdf0a7bbb19bab81a1d5b22c894cd649f4745c49efa6272edec7da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
